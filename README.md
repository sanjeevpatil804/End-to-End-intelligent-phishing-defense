# ğŸ›¡ï¸ End-to-End Intelligent Phishing Defense

[![GitHub](https://img.shields.io/badge/GitHub-Repository-blue)](https://github.com/sanjeevpatil804/End-to-End-intelligent-phishing-defense)
[![Python](https://img.shields.io/badge/Python-3.10-blue.svg)](https://www.python.org/)
[![FastAPI](https://img.shields.io/badge/FastAPI-Framework-green.svg)](https://fastapi.tiangolo.com/)
[![Docker](https://img.shields.io/badge/Docker-Enabled-blue.svg)](https://www.docker.com/)
[![AWS](https://img.shields.io/badge/AWS-Deployed-orange.svg)](https://aws.amazon.com/)

A production-ready machine learning system for intelligent phishing website detection using advanced feature engineering, automated hyperparameter optimization, and MLOps best practices. This end-to-end solution leverages 30 carefully engineered features to identify malicious URLs with high accuracy, deployed on AWS with CI/CD automation.

---

## ğŸ“‹ Table of Contents

- [Project Overview](#-project-overview)
- [Key Features](#-key-features)
- [Architecture](#-architecture)
- [Dataset](#-dataset)
- [Machine Learning Pipeline](#-machine-learning-pipeline)
- [Technology Stack](#-technology-stack)
- [Project Structure](#-project-structure)
- [Installation](#-installation)
- [Usage](#-usage)
- [API Documentation](#-api-documentation)
- [Model Performance](#-model-performance)
- [Deployment](#-deployment)
- [CI/CD Pipeline](#-cicd-pipeline)
- [Future Enhancements](#-future-enhancements)
- [Contributing](#-contributing)
- [License](#-license)

---

## ğŸ¯ Project Overview

Phishing attacks remain one of the most prevalent cybersecurity threats, with attackers continuously evolving their tactics to deceive users. This project implements an intelligent, automated defense system that:

- **Detects** phishing websites using machine learning with 30+ URL and content-based features
- **Processes** data through a robust ETL pipeline from MongoDB to trained models
- **Optimizes** model performance using Optuna for automated hyperparameter tuning
- **Deploys** seamlessly via Docker containers on AWS infrastructure
- **Automates** the entire ML lifecycle with CI/CD using GitHub Actions

The system achieves high precision and recall in detecting phishing attempts, making it suitable for integration into web browsers, security platforms, or standalone applications.

---

## âœ¨ Key Features

### ğŸ” **Intelligent Feature Engineering**
- **30 sophisticated features** extracted from URLs and webpage characteristics:
  - IP address detection in URLs
  - URL length analysis
  - URL shortening service detection
  - SSL certificate validation
  - Domain age and registration length
  - Web traffic patterns
  - Google indexing status
  - Statistical report analysis

### ğŸ¤– **Advanced Machine Learning**
- **Automated Model Selection**: Compares Random Forest and XGBoost classifiers
- **Optuna Hyperparameter Tuning**: 50+ trials for optimal parameter discovery
- **Cross-Validation**: Robust 5-fold CV for reliable performance estimation
- **Comprehensive Metrics**: F1-Score, Precision, Recall, and custom classification metrics

### ğŸ—ï¸ **Production-Ready MLOps**
- **Modular Pipeline Architecture**: Separate components for ingestion, validation, transformation, and training
- **Artifact Management**: Versioned model artifacts with timestamps
- **Model Registry**: Automated model versioning and metadata tracking
- **Exception Handling**: Custom exception framework for robust error management
- **Logging**: Comprehensive logging throughout the pipeline

### ğŸš€ **Cloud-Native Deployment**
- **FastAPI REST API**: High-performance async API for predictions
- **Docker Containerization**: Consistent deployment across environments
- **AWS ECR Integration**: Automated container registry management
- **GitHub Actions CI/CD**: Automated testing, building, and deployment
- **Scalable Infrastructure**: Self-hosted runner for continuous deployment

---

## ğŸ›ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Raw Data      â”‚
â”‚   (MongoDB)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Data Ingestion Component                    â”‚
â”‚  â€¢ Fetch data from MongoDB                              â”‚
â”‚  â€¢ Create train/test split                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Data Validation Component                      â”‚
â”‚  â€¢ Validate schema (30 features)                        â”‚
â”‚  â€¢ Check data quality                                   â”‚
â”‚  â€¢ Detect data drift                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        Data Transformation Component                     â”‚
â”‚  â€¢ KNN Imputation for missing values                    â”‚
â”‚  â€¢ Feature preprocessing pipeline                       â”‚
â”‚  â€¢ Save preprocessor object                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            Model Trainer Component                       â”‚
â”‚  â€¢ Optuna hyperparameter optimization                   â”‚
â”‚  â€¢ Random Forest & XGBoost comparison                   â”‚
â”‚  â€¢ Best model selection (F1-Score)                      â”‚
â”‚  â€¢ Model evaluation & reporting                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Prediction Pipeline                        â”‚
â”‚  â€¢ Load trained model & preprocessor                    â”‚
â”‚  â€¢ Batch prediction API endpoint                        â”‚
â”‚  â€¢ HTML table visualization                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Dataset

### Features (30 Input Variables)

The model uses 30 carefully engineered features to detect phishing websites:

| Feature Category | Features | Description |
|-----------------|----------|-------------|
| **URL-Based** | `having_IP_Address`, `URL_Length`, `Shortining_Service`, `having_At_Symbol`, `double_slash_redirecting`, `Prefix_Suffix` | Characteristics extracted directly from URL structure |
| **Domain-Based** | `having_Sub_Domain`, `SSLfinal_State`, `Domain_registeration_length`, `age_of_domain`, `DNSRecord` | Domain registration and SSL information |
| **Content-Based** | `Favicon`, `port`, `HTTPS_token`, `Request_URL`, `URL_of_Anchor`, `Links_in_tags`, `SFH`, `Submitting_to_email` | Webpage content analysis |
| **Behavioral** | `Abnormal_URL`, `Redirect`, `on_mouseover`, `RightClick`, `popUpWidnow`, `Iframe` | User interaction and behavioral patterns |
| **Reputation** | `web_traffic`, `Page_Rank`, `Google_Index`, `Links_pointing_to_page`, `Statistical_report` | External reputation metrics |

### Target Variable
- **`Result`**: Binary classification
  - `1`: Legitimate website
  - `-1` / `0`: Phishing website

### Data Source
- **Storage**: MongoDB (Cloud Database)
- **Database**: `SanjeevAI`
- **Collection**: `NetworkData`
- **Format**: CSV â†’ JSON â†’ MongoDB

---

## ğŸ”¬ Machine Learning Pipeline

### 1ï¸âƒ£ **Data Ingestion**
```python
- Connects to MongoDB using PyMongo
- Exports data to DataFrame
- Performs stratified train-test split (80-20)
- Saves raw data artifacts
```

### 2ï¸âƒ£ **Data Validation**
```python
- Validates schema against data_schema/schema.yaml
- Checks for required 30 features + target
- Ensures data types (all int64)
- Generates validation report
- Detects data drift
```

### 3ï¸âƒ£ **Data Transformation**
```python
- KNN Imputation for handling missing values
  â€¢ n_neighbors: 3
  â€¢ weights: 'uniform'
- Separates features and target
- Applies preprocessing pipeline
- Target encoding: -1 â†’ 0 for binary classification
- Saves preprocessor object for inference
```

### 4ï¸âƒ£ **Model Training with Optuna**
```python
- Automated hyperparameter optimization
  â€¢ Study Name: 'phishing_detection_study'
  â€¢ Trials: 50
  â€¢ Optimization Metric: F1-Score (CV)
  
- Classifiers Compared:
  1. Random Forest
     - n_estimators: [50, 300]
     - max_depth: [5, 30]
     - min_samples_split: [2, 20]
     
  2. XGBoost
     - n_estimators: [50, 300]
     - max_depth: [3, 15]
     - learning_rate: [0.01, 0.3]
     - subsample: [0.6, 1.0]

- Cross-Validation: 5-fold stratified
- Best model selection based on CV F1-score
- Comprehensive evaluation on train/test sets
```

### 5ï¸âƒ£ **Model Evaluation**
```python
Metrics Tracked:
- F1-Score (Primary metric)
- Precision
- Recall
- Confusion Matrix
- Classification Report

Reports Generated:
- model_evaluation_report.yaml
- Training metrics
- Test metrics
- Best hyperparameters
```

---

## ğŸ› ï¸ Technology Stack

### **Core Machine Learning**
- **Python 3.10**: Primary programming language
- **scikit-learn**: ML algorithms, preprocessing, metrics
- **XGBoost**: Gradient boosting classifier
- **Optuna**: Automated hyperparameter optimization
- **NumPy**: Numerical computations
- **Pandas**: Data manipulation and analysis

### **Data Storage & Management**
- **MongoDB**: Cloud database for raw data storage
- **PyMongo**: Python MongoDB driver
- **PyYAML**: Configuration management

### **Web Framework & API**
- **FastAPI**: Modern, high-performance web framework
- **Uvicorn**: ASGI server for FastAPI
- **Jinja2**: Template engine for HTML rendering
- **Starlette**: ASGI framework components
- **python-multipart**: File upload support

### **Deployment & DevOps**
- **Docker**: Containerization
- **AWS ECR**: Container registry
- **AWS EC2**: Self-hosted runner for deployment
- **GitHub Actions**: CI/CD automation
- **python-dotenv**: Environment variable management
- **certifi**: SSL certificate verification

### **Development Tools**
- **Git**: Version control
- **GitHub**: Code repository and CI/CD
- **VS Code**: Development environment

---

## ğŸ“ Project Structure

```
End-to-End-Network-Security/
â”‚
â”œâ”€â”€ app.py                          # FastAPI application entry point
â”œâ”€â”€ main.py                         # Training pipeline execution script
â”œâ”€â”€ push_data.py                    # MongoDB data ingestion utility
â”œâ”€â”€ requirements.txt                # Python dependencies
â”œâ”€â”€ Dockerfile                      # Container configuration
â”œâ”€â”€ Setup.py                        # Package setup configuration
â”œâ”€â”€ .env                            # Environment variables (MongoDB URL)
â”œâ”€â”€ .gitignore                      # Git ignore rules
â”‚
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ main.yaml               # CI/CD pipeline configuration
â”‚
â”œâ”€â”€ networksecurity/                # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ components/                 # Pipeline components
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py      # MongoDB â†’ CSV extraction
â”‚   â”‚   â”œâ”€â”€ Data_validation.py     # Schema & quality validation
â”‚   â”‚   â”œâ”€â”€ data_transformation.py # Preprocessing & imputation
â”‚   â”‚   â””â”€â”€ model_trainer.py       # Optuna-based model training
â”‚   â”‚
â”‚   â”œâ”€â”€ pipeline/                   # End-to-end pipelines
â”‚   â”‚   â””â”€â”€ training_pipeline.py   # Complete training workflow
â”‚   â”‚
â”‚   â”œâ”€â”€ entity/                     # Data classes
â”‚   â”‚   â”œâ”€â”€ config_entity.py       # Configuration dataclasses
â”‚   â”‚   â””â”€â”€ artifact_entity.py     # Artifact dataclasses
â”‚   â”‚
â”‚   â”œâ”€â”€ constants/                  # Constants & hyperparameters
â”‚   â”‚   â””â”€â”€ training_pipeline.py   # Pipeline constants
â”‚   â”‚
â”‚   â”œâ”€â”€ Exception/                  # Custom exceptions
â”‚   â”‚   â””â”€â”€ exception.py            # NetworkSecurityException
â”‚   â”‚
â”‚   â””â”€â”€ Utils/                      # Utility functions
â”‚       â””â”€â”€ main_utils/
â”‚           â”œâ”€â”€ utils.py            # General utilities
â”‚           â”œâ”€â”€ estimator.py        # NetworkModel class
â”‚           â”œâ”€â”€ classification_metrics.py  # Evaluation metrics
â”‚           â””â”€â”€ optuna_tuner.py     # Hyperparameter optimization
â”‚
â”œâ”€â”€ data_schema/
â”‚   â””â”€â”€ schema.yaml                 # Feature schema definition
â”‚
â”œâ”€â”€ Network_Data/
â”‚   â””â”€â”€ phishingData.csv            # Raw dataset
â”‚
â”œâ”€â”€ artifact/                       # Training artifacts (timestamped)
â”‚   â””â”€â”€ [timestamp]/
â”‚       â”œâ”€â”€ data_ingestion/
â”‚       â”œâ”€â”€ data_validation/
â”‚       â”œâ”€â”€ data_transformation/
â”‚       â””â”€â”€ model_trainer/
â”‚
â”œâ”€â”€ final_model/                    # Production models
â”‚   â”œâ”€â”€ model.pkl                   # Trained ML model
â”‚   â””â”€â”€ preprocessor.pkl            # Preprocessing pipeline
â”‚
â””â”€â”€ prediction_output/              # API prediction results
    â””â”€â”€ output.csv
```

---

## ğŸš€ Installation

### Prerequisites
- Python 3.10+
- MongoDB Atlas account (or local MongoDB)
- Docker (for containerization)
- AWS Account (for deployment)
- Git

### Local Setup

1. **Clone the Repository**
```bash
git clone https://github.com/sanjeevpatil804/End-to-End-intelligent-phishing-defense.git
cd End-to-End-intelligent-phishing-defense
```

2. **Create Virtual Environment**
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

3. **Install Dependencies**
```bash
pip install -r requirements.txt
```

4. **Configure Environment Variables**

Create a `.env` file in the root directory:
```env
MONGODB_URL_KEY=mongodb+srv://<username>:<password>@cluster.mongodb.net/
```

5. **Push Data to MongoDB** (First Time Only)
```bash
python push_data.py
```

---

## ğŸ’» Usage

### Training the Model

Run the complete training pipeline:

```bash
python main.py
```

This will execute:
1. Data ingestion from MongoDB
2. Data validation against schema
3. Data transformation with KNN imputation
4. Model training with Optuna optimization
5. Model evaluation and artifact generation

**Output:**
- Trained model saved in `artifact/[timestamp]/model_trainer/model.pkl`
- Final model saved in `final_model/model.pkl`
- Preprocessor saved in `final_model/preprocessor.pkl`
- Evaluation report in `artifact/[timestamp]/model_trainer/model_evaluation_report.yaml`

### Starting the API Server

Launch the FastAPI application:

```bash
python app.py
```

The API will be available at:
- **Swagger UI**: `http://localhost:8080/docs`
- **ReDoc**: `http://localhost:8080/redoc`

### Making Predictions

#### Via API (curl)
```bash
curl -X POST "http://localhost:8080/predict" \
  -H "accept: application/json" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@test_data.csv"
```

#### Via Python
```python
import requests

url = "http://localhost:8080/predict"
files = {"file": open("test_data.csv", "rb")}
response = requests.post(url, files=files)
print(response.text)
```

#### Via Swagger UI
1. Navigate to `http://localhost:8080/docs`
2. Click on `/predict` endpoint
3. Click "Try it out"
4. Upload CSV file with 30 features
5. Execute and view results

**Input CSV Format:**
```csv
having_IP_Address,URL_Length,Shortining_Service,...,Statistical_report
-1,54,1,...,1
1,20,-1,...,-1
```

**Output:**
- HTML table with predictions
- CSV file saved in `prediction_output/output.csv`
- `predicted_column`: 1 (legitimate) or -1 (phishing)

---

## ğŸ“¡ API Documentation

### Endpoints

#### **GET /**
- **Description**: Root endpoint, redirects to Swagger documentation
- **Response**: `302 Redirect` â†’ `/docs`

#### **POST /predict**
- **Description**: Batch prediction on uploaded CSV file
- **Request**:
  - **Type**: `multipart/form-data`
  - **Parameter**: `file` (CSV file)
- **Response**:
  - **Type**: `text/html`
  - **Body**: HTML table with predictions
  - **Side Effect**: Saves `prediction_output/output.csv`

**Example Response:**
```html
<table class="table table-striped">
  <thead>
    <tr><th>Feature1</th>...<th>predicted_column</th></tr>
  </thead>
  <tbody>
    <tr><td>-1</td>...<td>-1</td></tr>  <!-- Phishing -->
    <tr><td>1</td>...<td>1</td></tr>    <!-- Legitimate -->
  </tbody>
</table>
```

### CORS Configuration
- **Allowed Origins**: `*` (all origins)
- **Allowed Methods**: `*` (all HTTP methods)
- **Allowed Headers**: `*` (all headers)

---

## ğŸ“ˆ Model Performance

### Best Model Configuration

Based on 50 Optuna trials with 5-fold cross-validation:

**Selected Algorithm**: Random Forest / XGBoost (varies by run)

**Sample Performance Metrics**:

| Metric | Training Set | Test Set |
|--------|--------------|----------|
| **F1-Score** | 0.9845 | 0.9712 |
| **Precision** | 0.9821 | 0.9688 |
| **Recall** | 0.9869 | 0.9735 |
| **Accuracy** | 0.9850 | 0.9720 |

### Hyperparameter Optimization

The Optuna framework systematically searches the hyperparameter space:

```python
Search Space Example (Random Forest):
- n_estimators: [50, 300]
- max_depth: [5, 30]
- min_samples_split: [2, 20]
- min_samples_leaf: [1, 10]
- criterion: ['gini', 'entropy']

Optimization Process:
- Objective: Maximize F1-Score
- Cross-Validation: 5-fold stratified
- Number of Trials: 50
- Pruning: Enabled (MedianPruner)
```

### Model Artifacts

After training, the following artifacts are generated:

1. **model_evaluation_report.yaml**
```yaml
best_model: "random_forest" / "xgboost"
best_cv_f1_score: 0.9789
n_trials: 50
best_params:
  n_estimators: "200"
  max_depth: "15"
  min_samples_split: "5"
train_f1_score: 0.9845
train_precision: 0.9821
train_recall: 0.9869
test_f1_score: 0.9712
test_precision: 0.9688
test_recall: 0.9735
```

2. **Trained Model Files**
- `model.pkl`: Optimized classifier
- `preprocessor.pkl`: KNN imputer pipeline
- `NetworkModel`: Combined pipeline (preprocessor + model)

---

## ğŸŒ Deployment

### Docker Deployment

#### Build Docker Image
```bash
docker build -t phishing-detection:latest .
```

#### Run Container
```bash
docker run -d \
  -p 8080:8080 \
  --name phishing-api \
  -e MONGODB_URL_KEY="your_mongodb_connection_string" \
  phishing-detection:latest
```

#### Docker Compose (Optional)
```yaml
version: '3.8'
services:
  api:
    build: .
    ports:
      - "8080:8080"
    environment:
      - MONGODB_URL_KEY=${MONGODB_URL_KEY}
    restart: unless-stopped
```

### AWS Deployment Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GitHub Repo    â”‚â”€â”€â”€â”€â”€â–¶â”‚  GitHub Actions  â”‚â”€â”€â”€â”€â”€â–¶â”‚   AWS ECR       â”‚
â”‚  (Code Push)    â”‚      â”‚  (CI/CD)         â”‚      â”‚  (Docker Image) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                             â”‚
                                                             â–¼
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚     AWS EC2 (Self-Hosted Runner)         â”‚
                         â”‚  â€¢ Pulls latest image from ECR           â”‚
                         â”‚  â€¢ Stops old container                   â”‚
                         â”‚  â€¢ Runs new container on port 8080       â”‚
                         â”‚  â€¢ Cleans up old images                  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Deployment Steps

1. **AWS ECR Setup**
```bash
aws ecr create-repository --repository-name phishing-detection
aws ecr get-login-password --region us-east-1 | docker login --username AWS --password-stdin <account-id>.dkr.ecr.us-east-1.amazonaws.com
```

2. **GitHub Secrets Configuration**

Add the following secrets to your GitHub repository:
- `AWS_ACCESS_KEY_ID`
- `AWS_SECRET_ACCESS_KEY`
- `AWS_REGION`
- `ECR_REPOSITORY_NAME`
- `MONGODB_URL_KEY`

3. **Self-Hosted Runner Setup** (on AWS EC2)
```bash
# Install Docker
sudo yum update -y
sudo yum install docker -y
sudo service docker start

# Configure GitHub Runner
mkdir actions-runner && cd actions-runner
curl -o actions-runner-linux-x64-2.311.0.tar.gz -L https://github.com/actions/runner/releases/download/v2.311.0/actions-runner-linux-x64-2.311.0.tar.gz
tar xzf ./actions-runner-linux-x64-2.311.0.tar.gz
./config.sh --url https://github.com/sanjeevpatil804/End-to-End-intelligent-phishing-defense --token YOUR_TOKEN
./run.sh
```

---

## âš™ï¸ CI/CD Pipeline

### GitHub Actions Workflow

The `.github/workflows/main.yaml` defines a three-stage pipeline:

#### **Stage 1: Continuous Integration**
```yaml
- Checkout code from main branch
- Run linting checks
- Execute unit tests
- Validate code quality
```

#### **Stage 2: Continuous Delivery**
```yaml
- Install AWS CLI utilities
- Configure AWS credentials
- Login to Amazon ECR
- Build Docker image
- Tag image as 'latest'
- Push image to ECR repository
```

#### **Stage 3: Continuous Deployment**
```yaml
- Run on self-hosted EC2 runner
- Pull latest image from ECR
- Stop existing container (if running)
- Remove old container
- Run new container with environment variables
- Expose on port 8080
- Clean up unused images
```

### Trigger Conditions

The pipeline triggers on:
- **Push** to `main` branch
- **Excludes** changes to `README.md`

### Deployment Flow

```
Developer Push â†’ GitHub â†’ CI Tests â†’ Build Docker â†’ Push to ECR â†’ 
Deploy to EC2 â†’ Stop Old Container â†’ Start New Container â†’ Live
```

**Deployment Time**: ~5-8 minutes (from push to live)

---

## ğŸ”® Future Enhancements

### Model Improvements
- [ ] Implement deep learning models (LSTM, Transformer-based)
- [ ] Add real-time URL feature extraction API
- [ ] Ensemble methods with stacking/blending
- [ ] Online learning for continuous model updates
- [ ] A/B testing framework for model comparison

### Feature Engineering
- [ ] Extract additional features from DNS records
- [ ] Integrate VirusTotal API for reputation scores
- [ ] Add NLP features from page content analysis
- [ ] Implement WHOIS data integration
- [ ] Social media signals and brand verification

### Infrastructure & MLOps
- [ ] Implement model monitoring and drift detection
- [ ] Add Prometheus + Grafana for metrics
- [ ] Setup MLflow for experiment tracking
- [ ] Kubernetes deployment for auto-scaling
- [ ] Implement feature store (Feast/Tecton)
- [ ] Add model versioning and A/B deployment

### Application Features
- [ ] Real-time prediction API (single URL)
- [ ] Browser extension integration
- [ ] Email phishing detection module
- [ ] User feedback loop for model improvement
- [ ] Multi-language support for global URLs

### Data & Testing
- [ ] Expand dataset with recent phishing samples
- [ ] Implement data augmentation techniques
- [ ] Add comprehensive unit/integration tests
- [ ] Load testing and performance benchmarking
- [ ] Implement adversarial testing

---

## ğŸ¤ Contributing

Contributions are welcome! Please follow these steps:

1. **Fork the Repository**
```bash
git clone https://github.com/sanjeevpatil804/End-to-End-intelligent-phishing-defense.git
```

2. **Create a Feature Branch**
```bash
git checkout -b feature/your-feature-name
```

3. **Make Your Changes**
- Follow PEP 8 style guidelines
- Add docstrings to functions
- Update tests if applicable

4. **Commit Your Changes**
```bash
git add .
git commit -m "Add: Your descriptive commit message"
```

5. **Push to Your Fork**
```bash
git push origin feature/your-feature-name
```

6. **Create a Pull Request**
- Provide a clear description of changes
- Reference any related issues

### Code Quality Standards
- **Linting**: Ensure code passes flake8
- **Type Hints**: Use type annotations where possible
- **Documentation**: Update docstrings and README
- **Testing**: Add tests for new features

---

## ğŸ“„ License

This project is licensed under the **MIT License**.

```
MIT License

Copyright (c) 2025 Sanjeev Patil

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
```

---

## ğŸ‘¨â€ğŸ’» Author

**Sanjeev Patil**

- GitHub: [@sanjeevpatil804](https://github.com/sanjeevpatil804)
- LinkedIn: [Sanjeev Patil](https://www.linkedin.com/in/sanjeevpatil804)
- Email: sanjeevpatil804@aol.com

---

## ğŸ™ Acknowledgments

- **Dataset**: Phishing Website Dataset from UCI Machine Learning Repository
- **Frameworks**: FastAPI, scikit-learn, XGBoost, Optuna
- **Cloud Services**: MongoDB Atlas, AWS ECR/EC2
- **CI/CD**: GitHub Actions
- **Community**: Open-source contributors and cybersecurity researchers

---

## ğŸ“ Support

If you encounter any issues or have questions:

1. **Check Documentation**: Review this README and code comments
2. **Search Issues**: Look for similar issues in the GitHub repository
3. **Open an Issue**: Create a detailed issue with:
   - Problem description
   - Steps to reproduce
   - Expected vs actual behavior
   - Environment details (OS, Python version, etc.)

---

## â­ Star the Repository

If you find this project useful, please consider giving it a star â­ on GitHub!

---

<div align="center">

**Built with â¤ï¸ for a Safer Internet**

[ğŸ” Back to Top](#ï¸-end-to-end-intelligent-phishing-defense)

</div>
